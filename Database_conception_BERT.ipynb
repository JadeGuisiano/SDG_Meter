{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and create dataframe containing official objectives for each SDGs \n",
    "This page contains all the algorithms we used to increase the number of text in our initial database :\n",
    "\n",
    "- Natural Language Generation (NLG) - Markov Chain\n",
    "- Generate synonyms of each SDG keywords and get its definition with Wikipedia API\n",
    "- Presentation of a procedure to delete most common word in each SDG class text and none-sense word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG_Objectives contains all the targets for each SDG \n",
    "SDG_Obj = pd.read_csv('SDG_Objectives.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Nan cells by 0\n",
    "SDG_Obj = SDG_Obj.fillna(0)\n",
    "\n",
    "# Shuffle order to mix SDGs numbers\n",
    "random.seed(30)\n",
    "SDG_Obj = SDG_Obj.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>SDG_1</th>\n",
       "      <th>SDG_2</th>\n",
       "      <th>SDG_3</th>\n",
       "      <th>SDG_4</th>\n",
       "      <th>SDG_5</th>\n",
       "      <th>SDG_6</th>\n",
       "      <th>SDG_7</th>\n",
       "      <th>SDG_8</th>\n",
       "      <th>SDG_9</th>\n",
       "      <th>SDG_10</th>\n",
       "      <th>SDG_11</th>\n",
       "      <th>SDG_12</th>\n",
       "      <th>SDG_13</th>\n",
       "      <th>SDG_14</th>\n",
       "      <th>SDG_15</th>\n",
       "      <th>SDG_16</th>\n",
       "      <th>SDG_17_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>469</td>\n",
       "      <td>While there has been an increase in the number...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>550</td>\n",
       "      <td>11.7 By 2030, provide universal access to safe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               Text  SDG_1  SDG_2  \\\n",
       "468  469  While there has been an increase in the number...      0      0   \n",
       "549  550  11.7 By 2030, provide universal access to safe...      0      0   \n",
       "\n",
       "     SDG_3  SDG_4  SDG_5  SDG_6  SDG_7  SDG_8  SDG_9  SDG_10  SDG_11  SDG_12  \\\n",
       "468      0      0      0      0      0      0      1       0       0       0   \n",
       "549      0      0      0      0      0      0      0       0       1       0   \n",
       "\n",
       "     SDG_13  SDG_14  SDG_15  SDG_16  SDG_17_  \n",
       "468       0       0       0       0        0  \n",
       "549       0       0       0       0        0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change float format to int format \n",
    "cols=[i for i in SDG_Obj.columns if i not in [\"Text\"]]\n",
    "for col in cols:\n",
    "    SDG_Obj[col]=pd.to_numeric(SDG_Obj[col], downcast='integer')\n",
    "    \n",
    "# Check\n",
    "SDG_Obj.dtypes\n",
    "SDG_Obj.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Generation (Markov Chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select text for the 17 SDGs categories\n",
    "\n",
    "SDG_1 = SDG_Obj['SDG_1']==1\n",
    "SDG1 = SDG_Obj[SDG_1]\n",
    "SDG1 = SDG1.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_2 = SDG_Obj['SDG_2']==1\n",
    "SDG2 = SDG_Obj[SDG_2]\n",
    "SDG2 = SDG2.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_3 = SDG_Obj['SDG_3']==1\n",
    "SDG3 = SDG_Obj[SDG_3]\n",
    "SDG3 = SDG3.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_4 = SDG_Obj['SDG_4']==1\n",
    "SDG4 = SDG_Obj[SDG_4]\n",
    "SDG4 = SDG4.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_5 = SDG_Obj['SDG_5']==1\n",
    "SDG5 = SDG_Obj[SDG_5]\n",
    "SDG5 = SDG5.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_6 = SDG_Obj['SDG_6']==1\n",
    "SDG6 = SDG_Obj[SDG_6]\n",
    "SDG6 = SDG6.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_7 = SDG_Obj['SDG_7']==1\n",
    "SDG7 = SDG_Obj[SDG_7]\n",
    "SDG7 = SDG7.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_8 = SDG_Obj['SDG_8']==1\n",
    "SDG8 = SDG_Obj[SDG_8]\n",
    "SDG8 = SDG8.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_9 = SDG_Obj['SDG_9']==1\n",
    "SDG9 = SDG_Obj[SDG_9]\n",
    "SDG9 = SDG9.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_10 = SDG_Obj['SDG_10']==1\n",
    "SDG10 = SDG_Obj[SDG_10]\n",
    "SDG10 = SDG10.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_11 = SDG_Obj['SDG_11']==1\n",
    "SDG11 = SDG_Obj[SDG_11]\n",
    "SDG11 = SDG11.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_12 = SDG_Obj['SDG_12']==1\n",
    "SDG12 = SDG_Obj[SDG_12]\n",
    "SDG12 = SDG12.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_13 = SDG_Obj['SDG_13']==1\n",
    "SDG13 = SDG_Obj[SDG_13]\n",
    "SDG13 = SDG13.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_14 = SDG_Obj['SDG_14']==1\n",
    "SDG14 = SDG_Obj[SDG_14]\n",
    "SDG14 = SDG14.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_15 = SDG_Obj['SDG_15']==1\n",
    "SDG15 = SDG_Obj[SDG_15]\n",
    "SDG15 = SDG15.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_16 = SDG_Obj['SDG_16']==1\n",
    "SDG16 = SDG_Obj[SDG_16]\n",
    "SDG16 = SDG16.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)\n",
    "\n",
    "SDG_17_ = SDG_Obj['SDG_17_']==1\n",
    "SDG17 = SDG_Obj[SDG_17_]\n",
    "SDG17 = SDG17.drop(['ID', 'SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7',\n",
    "                 'SDG_8','SDG_9','SDG_10','SDG_11','SDG_12', 'SDG_13','SDG_14',\n",
    "                 'SDG_15','SDG_16','SDG_17_'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text for each SDG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "# Apply Markov Chain to original text\n",
    "\n",
    "text_model = markovify.NewlineText(SDG1.Text, state_size = 1)\n",
    "\n",
    "# Create a fonction \"f\" with make_sentence (Markovify) to genete text \n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_1 = f()\n",
    "#print(sentences_1)\n",
    "\n",
    "# convert list to dataframe\n",
    "S1 = pd.DataFrame(sentences_1) \n",
    "S1.columns = ['Text']\n",
    "\n",
    "# Assign 1 to all SGD1 texts\n",
    "S1['SDG_1'] = 1\n",
    "\n",
    "# 0 for others SDGs\n",
    "for newcol in ['SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S1[newcol]= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG2.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_2 = f()\n",
    "#print(sentences_2)\n",
    "\n",
    "# convert list to dataframe\n",
    "S2 = pd.DataFrame(sentences_2) \n",
    "S2.columns = ['Text']\n",
    "\n",
    "# 0 for others SDGs\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S2[newcol]= 0\n",
    "    \n",
    "# Assign 1 to all SGD2 texts\n",
    "S2['SDG_2'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG3.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_3 = f()\n",
    "#print(sentences_3)\n",
    "\n",
    "S3 = pd.DataFrame(sentences_3) \n",
    "S3.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S3[newcol]= 0\n",
    "    \n",
    "S3['SDG_3'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG4.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_4 = f()\n",
    "#print(sentences_4)\n",
    "\n",
    "S4 = pd.DataFrame(sentences_4)\n",
    "S4.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S4[newcol]= 0\n",
    "    \n",
    "S4['SDG_4'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG5.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_5 = f()\n",
    "#print(sentences_5)\n",
    "\n",
    "S5 = pd.DataFrame(sentences_5) \n",
    "S5.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S5[newcol]= 0\n",
    "    \n",
    "S5['SDG_5'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG6.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_6 = f()\n",
    "#print(sentences_6)\n",
    "\n",
    "S6 = pd.DataFrame(sentences_6) \n",
    "S6.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S6[newcol]= 0\n",
    "    \n",
    "S6['SDG_6'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG7.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_7 = f()\n",
    "#print(sentences_7)\n",
    "\n",
    "S7 = pd.DataFrame(sentences_7)\n",
    "S7.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S7[newcol]= 0\n",
    "    \n",
    "S7['SDG_7'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG8.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_8 = f()\n",
    "#print(sentences_8)\n",
    "\n",
    "S8 = pd.DataFrame(sentences_8)\n",
    "S8.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S8[newcol]= 0\n",
    "    \n",
    "S8['SDG_8'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG9.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_9 = f()\n",
    "#print(sentences_9)\n",
    "\n",
    "S9 = pd.DataFrame(sentences_9)\n",
    "S9.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S9[newcol]= 0\n",
    "    \n",
    "S9['SDG_9'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG10.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_10 = f()\n",
    "#print(sentences_10)\n",
    "\n",
    "S10 = pd.DataFrame(sentences_10)\n",
    "S10.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S10[newcol]= 0\n",
    "    \n",
    "S10['SDG_10'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG11.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_11 = f()\n",
    "#print(sentences_11)\n",
    "\n",
    "S11 = pd.DataFrame(sentences_11)\n",
    "S11.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S11[newcol]= 0\n",
    "    \n",
    "S11['SDG_11'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG12.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_12 = f()\n",
    "#print(sentences_12)\n",
    "\n",
    "S12 = pd.DataFrame(sentences_12)\n",
    "S12.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S12[newcol]= 0\n",
    "    \n",
    "S12['SDG_12'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG13.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_13 = f()\n",
    "#print(sentences_13)\n",
    "\n",
    "S13 = pd.DataFrame(sentences_13) \n",
    "S13.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S13[newcol]= 0\n",
    "    \n",
    "S13['SDG_13'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG14.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_14 = f()\n",
    "#print(sentences_14)\n",
    "\n",
    "S14 = pd.DataFrame(sentences_14)\n",
    "S14.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S14[newcol]= 0\n",
    "    \n",
    "S14['SDG_14'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG15.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_15 = f()\n",
    "#print(sentences_15)\n",
    "\n",
    "S15 = pd.DataFrame(sentences_15)\n",
    "S15.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S15[newcol]= 0\n",
    "    \n",
    "S15['SDG_15'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG16.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_16 = f()\n",
    "#print(sentences_16)\n",
    "\n",
    "S16 = pd.DataFrame(sentences_16)\n",
    "S16.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S16[newcol]= 0\n",
    "    \n",
    "S16['SDG_16'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = markovify.NewlineText(SDG17.Text, state_size = 1)\n",
    "\n",
    "def f():\n",
    "    result = [] \n",
    "    for i in range(300): # Generate 300 sentences \n",
    "        result.append(text_model.make_sentence())\n",
    "    return result\n",
    "\n",
    "sentences_17 = f()\n",
    "#print(sentences_17)\n",
    "\n",
    "S17 = pd.DataFrame(sentences_17) \n",
    "S17.columns = ['Text']\n",
    "\n",
    "for newcol in ['SDG_1','SDG_2','SDG_3','SDG_4','SDG_5','SDG_6','SDG_7', 'SDG_8','SDG_9','SDG_10',\n",
    "   'SDG_11','SDG_12', 'SDG_13','SDG_14','SDG_15','SDG_16','SDG_17_']:\n",
    "    S17[newcol]= 0\n",
    "    \n",
    "S17['SDG_17_'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concataining generated texts in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all the SGDs new text in a single dataframe \n",
    "New_text = pd.concat([S1, S2, S3, S4, S5, S6, S7, S8, S9, S10, S11, S12, S13, S14, S15, S16, S17], axis=0)\n",
    "New_text[\"ID\"] = np.arange(len(New_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataframe before split it in train test \n",
    "New_text = New_text.sample(frac=1)\n",
    "\n",
    "# Split New_text in train and test (80/20) sample (Sklearn function)\n",
    "#train, test = train_test_split(New_text, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_text['Text'] = New_text['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[i for i in New_text.columns if i not in [\"Text\"]]\n",
    "for col in cols:\n",
    "    New_text[col]=pd.to_numeric(New_text[col], downcast='integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatane SDG_objectives to Markov chain generated texts :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [SDG_Obj, New_text]\n",
    "res = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataframe before split it in train test \n",
    "#res = res.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"ID\"] = np.arange(len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to delete most common words between SDGs and none-sense words \n",
    "See script SDGs Text Analysis for word occurence for each SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG1\n",
    "SDG_1 = res['SDG_1']==1\n",
    "SDG1 = res[SDG_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['extreme', 'countries', 'world', 'population', 'development', 'including',\n",
    "                'paper', 'standard', 'national','well', 'basic', 'trends','africa', 'years',\n",
    "                'persons','declined', 'reduction', 'oecd','global','OECD','estimates','also',\n",
    "                'across','reduce','since','large','many','Africa','women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\b(?:extreme|countries|world|population|development|including|paper|standard|national|well|basic|trends|africa|years|persons|declined|reduction|oecd|global|OECD|estimates|also|across|reduce|since|large|many|Africa|women)\\\\b'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG1['Text'] = SDG1['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import collections\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "#Remove everythings exepct alphabet \n",
    "SDG1['clean_text'] = SDG1['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG1['clean_text'] = SDG1['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG1['clean_text'] =SDG1['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_1 = SDG1['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_1 = tokenized_doc_1.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_1.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('poverty', 692), ('social', 234), ('children', 161), ('protection', 154), ('living', 144), ('income', 125), ('people', 115), ('vulnerable', 91), ('poor', 89), ('child', 81), ('inequality', 72), ('benefits', 69), ('level', 57), ('levels', 56), ('access', 55), ('regions', 52), ('families', 52), ('agenda', 51), ('dimensions', 51), ('services', 50), ('support', 50), ('policy', 48), ('progress', 47), ('economic', 46), ('programmes', 46), ('cash', 46), ('monetary', 46), ('data', 45), ('significant', 45), ('receive', 45), ('forms', 44), ('adequate', 44), ('provide', 43), ('provides', 39), ('indicators', 39)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG2\n",
    "SDG_2 = res['SDG_2']==1\n",
    "SDG2 = res[SDG_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['million', 'countries', 'world', 'poverty', 'sustainable', 'people',\n",
    "               'access','children','development','systems','small','policy', 'cent',\n",
    "               'resilient','international','ensure','also','related','security','Targets'\n",
    "               'Climate','address','women','Change','global','agenda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG2['Text'] = SDG2['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG2['clean_text'] = SDG2['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG2['clean_text'] = SDG2['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG2['clean_text'] =SDG2['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_2 = SDG2['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_2 = tokenized_doc_2.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_2.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('food', 679), ('agriculture', 313), ('agricultural', 264), ('hunger', 236), ('including', 190), ('malnutrition', 160), ('resources', 127), ('production', 124), ('nutrition', 120), ('markets', 119), ('targets', 118), ('productivity', 107), ('efforts', 106), ('farmers', 101), ('climate', 99), ('rural', 99), ('water', 97), ('land', 94), ('genetic', 94), ('progress', 93), ('national', 92), ('productive', 92), ('producers', 92), ('many', 90), ('scale', 86), ('risk', 85), ('policies', 83), ('change', 81), ('achieving', 80), ('needed', 79), ('extreme', 78), ('overweight', 76), ('increased', 74), ('health', 73), ('increase', 71)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG3\n",
    "SDG_3 = res['SDG_3']==1\n",
    "SDG3 = res[SDG_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['people', 'countries', 'well', 'access', 'million', 'communicable',\n",
    "               'development','child','coverage','women','children','estimated', 'OECD',\n",
    "               'oecd','income','data','effective','universal','however','However'\n",
    "               'globally','rights','also','among','including','girls','Globally','Target','target',\n",
    "               'Targets','targets','Africa','Saharan','education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG3['Text'] = SDG3['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG3['clean_text'] = SDG3['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG3['clean_text'] = SDG3['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG3['clean_text'] =SDG3['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_3 = SDG3['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_3 = tokenized_doc_3.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_3.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('health', 875), ('diseases', 201), ('mental', 195), ('care', 192), ('deaths', 164), ('mortality', 159), ('global', 141), ('services', 139), ('burden', 94), ('maternal', 85), ('population', 80), ('progress', 79), ('risk', 76), ('reproductive', 73), ('still', 68), ('tuberculosis', 67), ('disease', 67), ('malaria', 64), ('available', 63), ('treatment', 62), ('births', 60), ('world', 60), ('however', 59), ('agenda', 59), ('system', 59), ('interventions', 59), ('public', 58), ('developing', 57), ('financing', 57), ('across', 56), ('conditions', 55), ('least', 55), ('number', 55), ('tobacco', 54), ('death', 53)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG4\n",
    "SDG_4 = res['SDG_4']==1\n",
    "SDG4 = res[SDG_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['women', 'gender', 'countrie', 'including', 'development', 'data', \n",
    "               'girls','implementation','access','global','national','early','Rights'\n",
    "               'care', 'human','also','provide','across','Saharan','Women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG4['Text'] = SDG4['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG4['clean_text'] = SDG4['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG4['clean_text'] = SDG4['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG4['clean_text'] =SDG4['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_4 = SDG4['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_4 = tokenized_doc_4.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_4.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('education', 581), ('learning', 217), ('countries', 182), ('children', 181), ('skills', 147), ('equality', 140), ('school', 133), ('secondary', 129), ('primary', 121), ('work', 90), ('quality', 90), ('vocational', 88), ('students', 84), ('training', 83), ('participation', 82), ('levels', 80), ('opportunities', 78), ('care', 74), ('labour', 73), ('sustainable', 72), ('educational', 69), ('support', 69), ('system', 69), ('adult', 67), ('many', 66), ('youth', 64), ('efforts', 63), ('ensure', 62), ('services', 62), ('health', 58), ('years', 58), ('reading', 57), ('united', 56), ('nations', 56), ('developing', 56)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG5\n",
    "SDG_5 = res['SDG_5']==1\n",
    "SDG5 = res[SDG_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['data', 'including', 'countries', 'implementation', 'development', 'health', \n",
    "                'across','access','care','national','resources','OECD','oecd','sustainable',\n",
    "                'action','services','policies','public','making','agenda','also','political',\n",
    "                'governments','level','levels','programme','positions','global',\n",
    "                'policy','CHAPTER','Chapter','chapter','mechanisms','average',\n",
    "                'international', 'Programme','decision','effective','Mexico'\n",
    "                'address','responsive','monitoring','agenda','private','continue','address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG5['Text'] = SDG5['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG5['clean_text'] = SDG5['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG5['clean_text'] = SDG5['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG5['clean_text'] =SDG5['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_5 = SDG5['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_5 = tokenized_doc_5.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_5.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('women', 862), ('gender', 486), ('girls', 304), ('equality', 288), ('violence', 151), ('work', 140), ('rights', 137), ('sexual', 111), ('unpaid', 104), ('social', 97), ('empowerment', 94), ('participation', 93), ('marriage', 77), ('domestic', 76), ('progress', 75), ('economic', 74), ('reproductive', 70), ('equal', 70), ('legal', 64), ('discrimination', 64), ('laws', 62), ('accountability', 61), ('leadership', 60), ('barriers', 59), ('human', 59), ('local', 59), ('gaps', 57), ('education', 57), ('efforts', 55), ('opportunities', 55), ('child', 55), ('protection', 54), ('achieve', 54), ('forms', 52), ('achieving', 52)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG6\n",
    "SDG_6 = res['SDG_6']==1\n",
    "SDG6 = res[SDG_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['management', 'protection', 'countries', 'related', 'climate', 'challenges', \n",
    "                'presents','billion','managed','central','government','affects','achieve',\n",
    "                'cooperation','governance','universal','facilities','efficiency','improve',\n",
    "                'globally','support','challenge','planning','security','Nothern','service',\n",
    "                'services','particular','public','human','natural','basic','development','world',\n",
    "                'sustainable','sector','CHAPTER','Chapter','chapter','including','including','change',\n",
    "                'population','also','Asia','levels','health','economic','increase','global',\n",
    "                'well','essential','Africa','need','growth','people'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG6['Text'] = SDG6['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG6['clean_text'] = SDG6['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG6['clean_text'] = SDG6['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG6['clean_text'] =SDG6['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_6 = SDG6['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_6 = tokenized_doc_6.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_6.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('water', 1037), ('sanitation', 225), ('wastewater', 107), ('quality', 95), ('resources', 94), ('drinking', 86), ('stress', 82), ('access', 68), ('supply', 66), ('availability', 60), ('irrigation', 57), ('ecosystems', 54), ('freshwater', 53), ('national', 52), ('treatment', 51), ('safely', 50), ('ensure', 48), ('transboundary', 43), ('recycling', 42), ('scarcity', 41), ('clean', 41), ('least', 36), ('france', 36), ('address', 35), ('integrated', 35), ('many', 34), ('local', 34), ('crucial', 34), ('hygiene', 33), ('northern', 33), ('infrastructure', 33), ('across', 33), ('life', 33), ('major', 32), ('impact', 32)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG7\n",
    "SDG_7 = res['SDG_7']==1\n",
    "SDG7 = res[SDG_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['countries', 'policies', 'regional', 'provides', 'climate', 'challenges', \n",
    "                'challenges','increased','infrastructure','services','solutions', 'cooperation',\n",
    "                'developing','improvement','universal','including','improved','international',\n",
    "                'generation','integrated','population','financial','continues','improvements',\n",
    "                'economic','policy','efficiency','CHAPTER','Chapter','chapter','sustainable'\n",
    "                'recent','change','development','billion','people','still','without','needs',\n",
    "                'however','Recent','years','private','since','issues','agenda','increase',\n",
    "                'technology','ensure','Agenda','however','However','also','share'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG7['Text'] = SDG7['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG7['clean_text'] = SDG7['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG7['clean_text'] = SDG7['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG7['clean_text'] =SDG7['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_7 = SDG7['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_7 = tokenized_doc_7.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_7.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('energy', 960), ('access', 205), ('renewable', 193), ('global', 185), ('investment', 130), ('growth', 129), ('clean', 107), ('electricity', 91), ('sustainable', 90), ('consumption', 87), ('rate', 84), ('modern', 84), ('power', 81), ('cooking', 74), ('fossil', 70), ('recent', 62), ('sector', 60), ('fuels', 54), ('resources', 54), ('technologies', 53), ('sectors', 53), ('progress', 50), ('solar', 48), ('security', 44), ('fuel', 42), ('measures', 42), ('cost', 42), ('transition', 42), ('support', 40), ('subsidies', 40), ('green', 39), ('trends', 39), ('country', 39), ('system', 37), ('wind', 36)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG8\n",
    "SDG_8 = res['SDG_8']==1\n",
    "SDG8 = res[SDG_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['countries','including','significant','inclusive','institutions','empirical',\n",
    "                'discusses','environment','enhancing','environments','infrastructure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG8['Text'] = SDG8['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG8['clean_text'] = SDG8['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG8['clean_text'] = SDG8['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG8['clean_text'] =SDG8['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_8 = SDG8['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_8 = tokenized_doc_8.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_8.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('growth', 283), ('employment', 257), ('labour', 246), ('work', 164), ('economic', 157), ('workers', 138), ('jobs', 126), ('productivity', 124), ('market', 121), ('opportunities', 121), ('social', 113), ('developed', 100), ('also', 98), ('policy', 91), ('world', 83), ('skills', 83), ('least', 82), ('chapter', 81), ('youth', 78), ('improve', 77), ('economy', 76), ('people', 73), ('rate', 71), ('education', 68), ('capita', 66), ('policies', 65), ('training', 65), ('young', 63), ('global', 63), ('financial', 63), ('quality', 63), ('working', 61), ('real', 61), ('increase', 60), ('high', 59)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG9\n",
    "SDG_9 = res['SDG_9']==1\n",
    "SDG9 = res[SDG_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['international','particular','respective','population',\n",
    "               'governments','additional','significantly','sustainable',\n",
    "               'development','country','countries','policy','policies','inclusive','increased','also','CHAPTER','Chapter',\n",
    "               'chapter','many','global','added','high','access','including','developed','social','well',\n",
    "               'challenges','however','However','discusses','least','growth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG9['Text'] = SDG9['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG9['clean_text'] = SDG9['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG9['clean_text'] = SDG9['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG9['clean_text'] =SDG9['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_9 = SDG9['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_9 = tokenized_doc_9.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_9.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('innovation', 323), ('infrastructure', 315), ('manufacturing', 171), ('economic', 153), ('developing', 143), ('support', 141), ('investment', 110), ('industrial', 108), ('industrialization', 104), ('business', 102), ('technologies', 99), ('research', 93), ('invention', 92), ('sectors', 90), ('value', 87), ('trade', 85), ('technological', 76), ('digital', 76), ('technology', 73), ('share', 73), ('firms', 72), ('goal', 72), ('process', 67), ('transformation', 66), ('industry', 64), ('provision', 64), ('promote', 62), ('made', 57), ('increase', 56), ('smes', 56), ('services', 55), ('green', 55), ('capabilities', 53), ('future', 51), ('resilient', 51)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG10\n",
    "SDG_10 = res['SDG_10']==1\n",
    "SDG10 = res[SDG_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['significantly','uncertainty','representation','population',\n",
    "               'countries','policies','OECD','developing','across','particular','paper',\n",
    "               'including','well','based','international','among','billion','Denmark','also',\n",
    "               'developed','development','world','ensure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG10['Text'] = SDG10['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG10['clean_text'] = SDG10['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG10['clean_text'] = SDG10['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG10['clean_text'] =SDG10['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_10 = SDG10['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_10 = tokenized_doc_10.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_10.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('income', 375), ('inequality', 293), ('growth', 181), ('social', 140), ('transfers', 113), ('impact', 111), ('reforms', 108), ('household', 107), ('distribution', 106), ('structural', 99), ('incomes', 96), ('global', 94), ('redistribution', 93), ('economic', 87), ('reduce', 80), ('average', 74), ('labour', 73), ('trade', 71), ('institutions', 65), ('decline', 63), ('assistance', 62), ('high', 62), ('mobility', 62), ('taxes', 62), ('least', 61), ('migration', 55), ('level', 54), ('inequalities', 53), ('market', 53), ('financial', 53), ('redistributive', 50), ('long', 48), ('goal', 46), ('higher', 45), ('benefits', 44)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG11\n",
    "SDG_11 = res['SDG_11']==1\n",
    "SDG11 = res[SDG_11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['substantially','implementation','highlighting','unprecedented', 'significance',\n",
    "               'agricultural','implementing','institutional','increasingly','intensifying','efficiencies',\n",
    "               'international','consequences','participation','nhistorically','sustainable','well','also',\n",
    "               'planning','Chapter','CHAPTER','chapter','billion','water','children','global','world','countries',\n",
    "               'inclusive','number','policies','economic','including','better','access','many','target',\n",
    "               'sector','goals','agenda','growth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG11['Text'] = SDG11['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG11['clean_text'] = SDG11['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG11['clean_text'] = SDG11['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG11['clean_text'] =SDG11['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_11 = SDG11['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_11 = tokenized_doc_11.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_11.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('urban', 560), ('cities', 311), ('development', 241), ('areas', 190), ('policy', 154), ('transport', 135), ('services', 130), ('housing', 127), ('waste', 126), ('rural', 123), ('people', 112), ('city', 110), ('management', 107), ('safe', 103), ('environmental', 98), ('urbanization', 97), ('population', 91), ('public', 91), ('persons', 80), ('capital', 79), ('state', 68), ('examines', 67), ('progress', 65), ('regional', 63), ('human', 59), ('affordable', 59), ('basic', 58), ('indicators', 58), ('attention', 57), ('increased', 55), ('rapid', 54), ('infrastructure', 54), ('reviews', 53), ('framework', 52), ('quality', 52)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG12\n",
    "SDG_12 = res['SDG_12']==1\n",
    "SDG12 = res[SDG_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['international','substantially','significantly','fundamentally','countries',\n",
    "               'disadvantaged','macroeconomic','transnational','participation','development',\n",
    "               'management','natural','Chapter','CHAPTER','chapter','related','policies','developing',\n",
    "               'well','value','growth','policy','national','frameworks','needs','global','towards'\n",
    "               'provides','efficiency','along']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG12['Text'] = SDG12['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG12['clean_text'] = SDG12['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG12['clean_text'] = SDG12['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG12['clean_text'] =SDG12['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_12 = SDG12['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_12 = tokenized_doc_12.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_12.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('waste', 275), ('consumption', 199), ('sustainable', 190), ('production', 179), ('material', 178), ('food', 147), ('economy', 145), ('circular', 131), ('resources', 131), ('economic', 130), ('towards', 115), ('environmental', 102), ('resource', 96), ('transition', 86), ('materials', 82), ('sustainability', 79), ('footprint', 76), ('tons', 67), ('impacts', 63), ('trade', 59), ('practices', 56), ('products', 55), ('quality', 55), ('efficient', 55), ('used', 52), ('chemicals', 51), ('life', 50), ('recycling', 49), ('patterns', 49), ('reduction', 47), ('guidance', 46), ('billion', 46), ('extraction', 44), ('capita', 43), ('amount', 43)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(35)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG13\n",
    "SDG_13 = res['SDG_13']==1\n",
    "SDG13 = res[SDG_13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['countries','development','global','national','developing','developed',\n",
    "               'levels','well','least','governments','Chapter','chapter','CHAPTER','OECD','related','Framework','parties','states',\n",
    "               'many','million','sustainable','framework','policies','addition','women','often','particularly',\n",
    "               'current','infrastructure','government','much','April','operation','article','people','nationally','address','Current']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG13['Text'] = SDG13['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG13['clean_text'] = SDG13['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG13['clean_text'] = SDG13['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG13['clean_text'] =SDG13['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_13 = SDG13['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_13 = tokenized_doc_13.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_13.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('climate', 618), ('change', 391), ('disasters', 145), ('adaptation', 135), ('agreement', 116), ('action', 114), ('risk', 112), ('natural', 99), ('disaster', 97), ('emissions', 85), ('paris', 85), ('resilience', 82), ('impact', 80), ('strategies', 79), ('resilient', 69), ('actions', 67), ('hazards', 66), ('reduction', 60), ('capacity', 56), ('warming', 56), ('united', 51), ('nations', 51), ('convention', 51), ('mitigation', 50), ('planning', 45), ('states', 45), ('increase', 45), ('report', 45), ('vulnerable', 44), ('help', 43), ('relationship', 42), ('greenhouse', 41), ('effects', 40), ('goals', 40), ('goal', 40), ('small', 39), ('looks', 38), ('first', 38), ('management', 37), ('island', 37), ('local', 37), ('sector', 37), ('implement', 37), ('carbon', 37), ('reduce', 37), ('programmes', 37), ('pathways', 37), ('adaptive', 36), ('determined', 36), ('contributions', 36)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(50)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG14\n",
    "SDG_14 = res['SDG_14']==1\n",
    "SDG14 = res[SDG_14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['sustainable','countries','small','well','development','global','role','international','International',\n",
    "               'management','developed','levels','least','within','scale','contribute','developing','economics','trends','increase',\n",
    "               'states','implementation','health','based','year','also','economic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG14['Text'] = SDG14['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG14['clean_text'] = SDG14['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG14['clean_text'] = SDG14['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG14['clean_text'] =SDG14['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_14 = SDG14['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_14 = tokenized_doc_14.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_14.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('marine', 477), ('oceans', 159), ('pollution', 158), ('ocean', 150), ('fisheries', 143), ('food', 130), ('resources', 128), ('fishing', 124), ('coastal', 115), ('fish', 97), ('biodiversity', 91), ('conservation', 81), ('stocks', 66), ('land', 62), ('areas', 54), ('life', 54), ('subsidies', 53), ('ecosystems', 53), ('seas', 52), ('livelihoods', 51), ('unregulated', 49), ('including', 47), ('water', 46), ('order', 43), ('indonesia', 42), ('webs', 42), ('small', 41), ('states', 41), ('illegal', 41), ('unreported', 41), ('current', 40), ('waters', 40), ('sector', 40), ('security', 40), ('island', 39), ('chapter', 39), ('earth', 38), ('world', 38), ('policy', 37), ('since', 37), ('policies', 37), ('address', 37), ('provides', 37), ('increased', 36), ('industrial', 36), ('large', 35), ('increasing', 35), ('depend', 35), ('convention', 35), ('biotechnology', 35)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(50)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG15\n",
    "SDG_15 = res['SDG_15']==1\n",
    "SDG15 = res[SDG_15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['countries','water','development','across','services','management','including',\n",
    "               'chapter','Chapter','CHAPTER','covered','benefits','support','also','important','trades','national','food',\n",
    "               'approaches','continues','policies','million','sector','global','international','policy','world','trade',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG15['Text'] = SDG15['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG15['clean_text'] = SDG15['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG15['clean_text'] = SDG15['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG15['clean_text'] =SDG15['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_15 = SDG15['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_15 = tokenized_doc_15.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_15.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('biodiversity', 469), ('forest', 306), ('species', 235), ('land', 227), ('areas', 207), ('ecosystems', 197), ('wildlife', 187), ('sustainable', 162), ('loss', 160), ('forests', 142), ('conservation', 137), ('terrestrial', 107), ('resources', 102), ('ecosystem', 95), ('degradation', 94), ('trafficking', 91), ('protected', 76), ('poaching', 74), ('trends', 68), ('environmental', 67), ('area', 65), ('protecting', 65), ('provide', 65), ('mountain', 64), ('deforestation', 63), ('economic', 62), ('progress', 60), ('natural', 60), ('green', 60), ('earth', 59), ('restore', 58), ('increased', 58), ('well', 58), ('protect', 57), ('restoration', 54), ('instruments', 53), ('illegal', 53), ('hectares', 53), ('many', 52), ('however', 51), ('diversity', 50), ('related', 50), ('illicit', 49), ('action', 49), ('efforts', 48), ('promote', 47), ('degraded', 47), ('ensure', 47), ('private', 47), ('communities', 46)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(50)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG16\n",
    "SDG_16 = res['SDG_16']==1\n",
    "SDG16 = res[SDG_16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['countries','development','access','framework','inclusive','sustainable','developing','also','Chapter',\n",
    "               'effective','people','international','levels','report','Chapter','children','human', 'services',\n",
    "               'ensure','data','national','public','growth','making','CHAPTER','many','OECD','based','Africa'\n",
    "               'agenda','Agenda','well','number','increased','involved','important','strengthen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG16['Text'] = SDG16['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG16['clean_text'] = SDG16['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG16['clean_text'] = SDG16['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG16['clean_text'] =SDG16['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_16 = SDG16['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_16 = tokenized_doc_16.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_16.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('justice', 200), ('corruption', 164), ('violence', 151), ('institutions', 139), ('legal', 137), ('rights', 111), ('peace', 108), ('conflict', 85), ('africa', 77), ('homicide', 75), ('forms', 73), ('foreign', 70), ('laws', 69), ('progress', 69), ('regions', 66), ('accountable', 65), ('illicit', 65), ('including', 63), ('security', 63), ('societies', 59), ('information', 58), ('available', 58), ('different', 57), ('implementation', 57), ('years', 56), ('flows', 52), ('role', 52), ('without', 51), ('crime', 51), ('judge', 51), ('strengthening', 50), ('actors', 49), ('policies', 48), ('economies', 46), ('goal', 45), ('decision', 45), ('armed', 45), ('civil', 45), ('society', 45), ('governance', 44), ('economic', 43), ('increasing', 43), ('violent', 42), ('could', 42), ('needs', 42), ('remained', 41), ('financial', 41), ('efforts', 41), ('convention', 40), ('rule', 40)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(50)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG17\n",
    "SDG_17 = res['SDG_17_']==1\n",
    "SDG17 = res[SDG_17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to be delete \n",
    "remove_words = ['countries','sustainable','least','developed','developing','enhance','based','international','policy',\n",
    "               'including','national','chapter','agenda','global','world','increase','effective','South','available','capacity'\n",
    "                ,'data','Sustainable','Agenda','level','billion','Enhance','applied','particular',\n",
    "               'existing','Asia','need','chapter','Chapter','country','However','however']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = r'\\b(?:{})\\b'.format('|'.join(remove_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "SDG17['Text'] = SDG17['Text'].str.replace(pat, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jguisiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove everythings exepct alphabet \n",
    "SDG17['clean_text'] = SDG17['Text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# Remove null fields\n",
    "SDG17['clean_text'] = SDG17['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Make all text lowercase\n",
    "SDG17['clean_text'] =SDG17['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenization\n",
    "tokenized_doc_17 = SDG17['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Delete stop-words\n",
    "nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['cent']\n",
    "stop_words.extend(newStopWords)\n",
    "tokenized_doc_17 = tokenized_doc_17.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = tokenized_doc_17.tolist()\n",
    "\n",
    "new_list = []\n",
    "for words in list:\n",
    "    new_list += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('development', 421), ('financing', 116), ('support', 112), ('partnerships', 112), ('science', 103), ('goals', 102), ('technology', 85), ('triangular', 75), ('partnership', 73), ('operation', 72), ('actors', 70), ('assistance', 64), ('building', 58), ('debt', 57), ('domestic', 55), ('registration', 54), ('finance', 53), ('financial', 52), ('resources', 52), ('factors', 52), ('among', 52), ('knowledge', 52), ('statistical', 51), ('public', 51), ('united', 50), ('plans', 49), ('share', 49), ('income', 48), ('trade', 48), ('cooperation', 47), ('practical', 47), ('achieve', 47), ('around', 46), ('well', 46), ('action', 43), ('partners', 42), ('instruments', 42), ('private', 41), ('costs', 41), ('high', 40), ('inclusive', 39), ('means', 38), ('sources', 38), ('gross', 37), ('implementation', 37), ('policies', 37), ('strategies', 37), ('promote', 36), ('also', 35), ('implement', 35)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(new_list)\n",
    "top = word_counts.most_common(50)\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all modified text (SDG1 ...) and delete text_clean for all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all the SDG in a single dataframe \n",
    "res = pd.concat([SDG1, SDG2, SDG3, SDG4, SDG5, SDG6, SDG7, SDG8, SDG9, SDG10, SDG11, SDG12, SDG13, SDG14, SDG15, SDG16, SDG17], axis=0)\n",
    "res = res.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"ID\"] = np.arange(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['Text'] = res['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.drop('clean_text', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.Text = res.Text.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6016 entries, 203 to 50\n",
      "Data columns (total 19 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   ID       6016 non-null   int64 \n",
      " 1   Text     6016 non-null   object\n",
      " 2   SDG_1    6016 non-null   int8  \n",
      " 3   SDG_2    6016 non-null   int8  \n",
      " 4   SDG_3    6016 non-null   int8  \n",
      " 5   SDG_4    6016 non-null   int8  \n",
      " 6   SDG_5    6016 non-null   int8  \n",
      " 7   SDG_6    6016 non-null   int8  \n",
      " 8   SDG_7    6016 non-null   int8  \n",
      " 9   SDG_8    6016 non-null   int8  \n",
      " 10  SDG_9    6016 non-null   int8  \n",
      " 11  SDG_10   6016 non-null   int8  \n",
      " 12  SDG_11   6016 non-null   int8  \n",
      " 13  SDG_12   6016 non-null   int8  \n",
      " 14  SDG_13   6016 non-null   int8  \n",
      " 15  SDG_14   6016 non-null   int8  \n",
      " 16  SDG_15   6016 non-null   int8  \n",
      " 17  SDG_16   6016 non-null   int8  \n",
      " 18  SDG_17_  6016 non-null   int8  \n",
      "dtypes: int64(1), int8(17), object(1)\n",
      "memory usage: 240.9+ KB\n"
     ]
    }
   ],
   "source": [
    "res.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the synonyms of each SDG keywords and obtains its definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Generate synonyms for each SDG keywords with GTP-2\n",
    "import requests\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/text-generator\",\n",
    "    data={\n",
    "        'text': 'End poverty, protection for the poor',\n",
    "    },\n",
    "    headers={'api-key': 'quickstart-QUdJIGlzIGNvbWluZy4uLi4K'}\n",
    ")\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wikipedia articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search synonyms for each SDG keywords by using NLTK\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD1 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Poverty\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD1 = set(synonyms)\n",
    "SD1 = pd.DataFrame(SD1) \n",
    "SD1.columns = ['text']\n",
    "SD1['col'] = 'SDG1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD2 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Hunger\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD2 = set(synonyms)\n",
    "SD2 = pd.DataFrame(SD2) \n",
    "SD2.columns = ['text']\n",
    "SD2['col'] = 'SDG2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD3 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Health\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD3 = set(synonyms)\n",
    "SD3 = pd.DataFrame(SD3) \n",
    "SD3.columns = ['text']\n",
    "SD3['col'] = 'SDG3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD31 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Well-being\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD31 = set(synonyms)\n",
    "SD31 = pd.DataFrame(SD31) \n",
    "SD31.columns = ['text']\n",
    "SD31['col'] = 'SDG3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD4 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Education\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD4 = set(synonyms)\n",
    "SD4 = pd.DataFrame(SD4) \n",
    "SD4.columns = ['text']\n",
    "SD4['col'] = 'SDG4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD5 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Gender\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD5 = set(synonyms)\n",
    "SD5 = pd.DataFrame(SD5) \n",
    "SD5.columns = ['text']\n",
    "SD5['col'] = 'SDG5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD6 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Sanitation\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD6 = set(synonyms)\n",
    "SD6 = pd.DataFrame(SD6) \n",
    "SD6.columns = ['text']\n",
    "SD6['col'] = 'SDG6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD61 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Water\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD61 = set(synonyms)\n",
    "SD61 = pd.DataFrame(SD61) \n",
    "SD61.columns = ['text']\n",
    "SD61['col'] = 'SDG6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD7 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Energy\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD7 = set(synonyms)\n",
    "SD7 = pd.DataFrame(SD7)\n",
    "SD7.columns = ['text']\n",
    "SD7['col'] = 'SDG7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD8 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Economy\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD8 = set(synonyms)\n",
    "SD8 = pd.DataFrame(SD8) \n",
    "SD8.columns = ['text']\n",
    "SD8['col'] = 'SDG8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD81 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Work\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD81 = set(synonyms)\n",
    "SD81 = pd.DataFrame(SD81) \n",
    "SD81.columns = ['text']\n",
    "SD81['col'] = 'SDG8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD9 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Industry\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD9 = set(synonyms)\n",
    "SD9 = pd.DataFrame(SD9) \n",
    "SD9.columns = ['text']\n",
    "SD9['col'] = 'SDG9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD91 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Innovation\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD91 = set(synonyms)\n",
    "SD91 = pd.DataFrame(SD91) \n",
    "SD91.columns = ['text']\n",
    "SD91['col'] = 'SDG9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD92 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Infrastructure\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD92 = set(synonyms)\n",
    "SD92 = pd.DataFrame(SD92) \n",
    "SD92.columns = ['text']\n",
    "SD92['col'] = 'SDG9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD10 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Inequality\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD10 = set(synonyms)\n",
    "SD10 = pd.DataFrame(SD10) \n",
    "SD10.columns = ['text']\n",
    "SD10['col'] = 'SDG10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD11 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Cities\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD11 = set(synonyms)\n",
    "SD11 = pd.DataFrame(SD11) \n",
    "SD11.columns = ['text']\n",
    "SD11['col'] = 'SDG11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD111 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Communities\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD111 = set(synonyms)\n",
    "SD111 = pd.DataFrame(SD111) \n",
    "SD111.columns = ['text']\n",
    "SD111['col'] = 'SDG11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD12 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Consumption\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD12 = set(synonyms)\n",
    "SD12 = pd.DataFrame(SD12) \n",
    "SD12.columns = ['text']\n",
    "SD12['col'] = 'SDG12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD121 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Production\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD121 = set(synonyms)\n",
    "SD121 = pd.DataFrame(SD121) \n",
    "SD121.columns = ['text']\n",
    "SD121['col'] = 'SDG12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD13 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Climate\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD13 = set(synonyms)\n",
    "SD13 = pd.DataFrame(SD13) \n",
    "SD13.columns = ['text']\n",
    "SD13['col'] = 'SDG13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD14 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Marine\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD14 = set(synonyms)\n",
    "SD14 = pd.DataFrame(SD14) \n",
    "SD14.columns = ['text']\n",
    "SD14['col'] = 'SDG14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD141 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Coastal\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD141 = set(synonyms)\n",
    "SD141 = pd.DataFrame(SD141) \n",
    "SD141.columns = ['text']\n",
    "SD141['col'] = 'SDG14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD15 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Terrestrial\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD15 = set(synonyms)\n",
    "SD15 = pd.DataFrame(SD15) \n",
    "SD15.columns = ['text']\n",
    "SD15['col'] = 'SDG15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD151 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Forests\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD151 = set(synonyms)\n",
    "SD151 = pd.DataFrame(SD151) \n",
    "SD151.columns = ['text']\n",
    "SD151['col'] = 'SDG15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD152 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Biodiversity\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD152 = set(synonyms)\n",
    "SD152 = pd.DataFrame(SD152) \n",
    "SD152.columns = ['text']\n",
    "SD152['col'] = 'SDG15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD153 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"land\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD153 = set(synonyms)\n",
    "SD153 = pd.DataFrame(SD153) \n",
    "SD153.columns = ['text']\n",
    "SD153['col'] = 'SDG15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD16 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Peace\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD16 = set(synonyms)\n",
    "SD16 = pd.DataFrame(SD16) \n",
    "SD16.columns = ['text']\n",
    "SD16['col'] = 'SDG16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD161 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Justice\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD161 = set(synonyms)\n",
    "SD161 = pd.DataFrame(SD161) \n",
    "SD161.columns = ['text']\n",
    "SD161['col'] = 'SDG16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD17 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Trade\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD17 = set(synonyms)\n",
    "SD17 = pd.DataFrame(SD17) \n",
    "SD17.columns = ['text']\n",
    "SD17['col'] = 'SDG17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD171 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Technology\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD171 = set(synonyms)\n",
    "SD171 = pd.DataFrame(SD171) \n",
    "SD171.columns = ['text']\n",
    "SD171['col'] = 'SDG17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD172 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Finance\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD172 = set(synonyms)\n",
    "SD172 = pd.DataFrame(SD172) \n",
    "SD172.columns = ['text']\n",
    "type(SD172)\n",
    "SD172['col'] = 'SDG17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = [] \n",
    "antonyms = [] \n",
    "SD173 =[] \n",
    "\n",
    "for syn in wordnet.synsets(\"Partnership\"): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "        if l.antonyms(): \n",
    "            antonyms.append(l.antonyms()[0].name()) \n",
    "  \n",
    "SD173 = set(synonyms)\n",
    "SD173 = pd.DataFrame(SD173) \n",
    "SD173.columns = ['text']\n",
    "SD173['col'] = 'SDG17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all in a single dataframe \n",
    "Syn = pd.concat([SD1, SD2, SD3, SD31, SD4, SD5, SD6, SD61, SD7, SD8, SD81, SD9, SD91, SD92, SD10, SD11, SD111, SD12, SD121, SD13, SD14, SD141, SD15, SD151, SD152, SD153, SD16, SD161, SD17, SD171, SD172, SD173], axis=0)\n",
    "Syn[\"ID\"] = np.arange(len(Syn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas.set_option('display.max_rows', 300)\n",
    "Syn.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save syn in CSV and manually drop unuseful synonyms for each SDG\n",
    "#Syn.to_csv('Syn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean Syn CSV file in dataframe\n",
    "Syn = pd.read_csv('Syn.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Synonyms = Syn['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a list of synonyms and use it in a loop for wikpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "Def = []\n",
    "for i in Synonyms:\n",
    "    Def.append((i, wikipedia.summary(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Def was exported to CSV file, then we also filter his results and add it to the initial database SDG_Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF text extraction \n",
    "We also add a selection of PDF files for each SDG sort by UN expert that we added in our database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser \n",
    "# Extract PDF text \n",
    "raw = parser.from_file('Flagship reports for SDG analysis/SDG 1 - No poverty/2017 HLPF Thematic review SDG1.pdf')\n",
    "SDG1_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG1_1 = SDG1_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG1_1 = [x for x in SDG1_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG1_1 = map(lambda SDG1_1:SDG1_1.strip(' '' '),SDG1_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG1_1 = \" \".join(SDG1_1)\n",
    "\n",
    "# Delete extra white space \n",
    "import re\n",
    "SDG1_1 = re.sub(r' +', ' ', SDG1_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG1_1 = re.sub(r\"http\\S+\", \"\", SDG1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar way to do whihtout the need of Java\n",
    "# Loop for extracting all pages text for each document : \n",
    "\n",
    "#import PyPDF2 as pdf\n",
    "\n",
    "# Check if PDF is encrypted \n",
    "#read_pdf.getIsEncrypted()\n",
    "\n",
    "#with open('Flagship reports for SDG analysis/SDG 1 - No poverty/2017 HLPF Thematic review SDG1.pdf','rb') as pdf_file, open('SDG1_1.txt', 'w') as text_file:\n",
    "    #read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "    #number_of_pages = read_pdf.getNumPages()\n",
    "    #for page_number in range(number_of_pages):  \n",
    "        #page = read_pdf.getPage(page_number)\n",
    "        #page_content = page.extractText()\n",
    "        #text_file.write(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 1 - No poverty/.Global multidimensional povery index 2019.pdf.icloud')\n",
    "SDG1_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG1_2 = SDG1_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG1_2 = [x for x in SDG1_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG1_2 = map(lambda SDG1_2:SDG1_2.strip(' '' '),SDG1_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG1_2 = \" \".join(SDG1_2)\n",
    "\n",
    "# Delete extra white space \n",
    "import re\n",
    "SDG1_2 = re.sub(r' +', ' ', SDG1_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG1_2 = re.sub(r\"http\\S+\", \"\", SDG1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 2- No hunger/2017 HLPF thematic review SDG2.pdf')\n",
    "SDG2_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG2_1 = SDG2_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG2_1 = [x for x in SDG2_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG2_1 = map(lambda SDG2_1:SDG2_1.strip(' '' '),SDG2_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG2_1 = \" \".join(SDG2_1)\n",
    "\n",
    "# Delete extra white space \n",
    "import re\n",
    "SDG2_1 = re.sub(r' +', ' ', SDG2_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG2_1 = re.sub(r\"http\\S+\", \"\", SDG2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 2- No hunger/.The State of Food Security and Nutrition in the World.pdf.icloud')\n",
    "SDG2_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG2_2 = SDG2_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG2_2 = [x for x in SDG2_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG2_2 = map(lambda SDG2_2:SDG2_2.strip(' '' '),SDG2_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG2_2 = \" \".join(SDG2_2)\n",
    "\n",
    "# Delete extra white space \n",
    "import re\n",
    "SDG2_2 = re.sub(r' +', ' ', SDG2_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG2_2 = re.sub(r\"http\\S+\", \"\", SDG2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 3 - Health/2017 HLPF Thematic review SDG 3.pdf')\n",
    "SDG3_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG3_1 = SDG3_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG3_1 = [x for x in SDG3_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG3_1 = map(lambda SDG3_1:SDG3_1.strip(' '' '),SDG3_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG3_1 = \" \".join(SDG3_1)\n",
    "\n",
    "# Delete extra white space \n",
    "import re\n",
    "SDG3_1 = re.sub(r' +', ' ', SDG3_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG3_1 = re.sub(r\"http\\S+\", \"\", SDG3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 4 - Education/.Global Education Monitoring report 2020 UNESCO.pdf.icloud')\n",
    "SDG4_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG4_1 = SDG4_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG4_1 = [x for x in SDG4_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG4_1 = map(lambda SDG4_1:SDG4_1.strip(' '' '),SDG4_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG4_1 = \" \".join(SDG4_1)\n",
    "\n",
    "# Delete extra white space \n",
    "import re\n",
    "SDG4_1 = re.sub(r' +', ' ', SDG4_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG4_1 = re.sub(r\"http\\S+\", \"\", SDG4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 5 - Gender Equality/HLPF 2017 thematic review sdg 5.pdf')\n",
    "SDG5_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG5_1 = SDG5_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG5_1 = [x for x in SDG5_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG5_1 = map(lambda SDG5_1:SDG5_1.strip(' '' '),SDG5_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG5_1 = \" \".join(SDG5_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG5_1 = re.sub(r' +', ' ', SDG5_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG5_1 = re.sub(r\"http\\S+\", \"\", SDG5_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 5 - Gender Equality/HLPF 2017 thematic review sdg 5.pdf')\n",
    "SDG6_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG6_1 = SDG6_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG6_1 = [x for x in SDG6_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG6_1 = map(lambda SDG6_1:SDG6_1.strip(' '' '),SDG6_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG6_1 = \" \".join(SDG6_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG6_1 = re.sub(r' +', ' ', SDG6_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG6_1 = re.sub(r\"http\\S+\", \"\", SDG6_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 6 - Water/Water and climate change_UN Water.pdf')\n",
    "SDG6_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG6_2 = SDG6_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG6_2 = [x for x in SDG6_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG6_2 = map(lambda SDG6_2:SDG6_2.strip(' '' '),SDG6_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG6_2 = \" \".join(SDG6_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG6_2 = re.sub(r' +', ' ', SDG6_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG6_2 = re.sub(r\"http\\S+\", \"\", SDG6_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 7 - Energy/HLPF Thematic review SDG 7.pdf')\n",
    "SDG7_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG7_1 = SDG7_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG7_1 = [x for x in SDG7_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG7_1 = map(lambda SDG7_1:SDG7_1.strip(' '' '),SDG7_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG7_1 = \" \".join(SDG7_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG7_1 = re.sub(r' +', ' ', SDG7_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG7_1 = re.sub(r\"http\\S+\", \"\", SDG7_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 7 - Energy/.Policy Briefs in support of the first SDG7 review at hlpf.pdf.icloud')\n",
    "SDG7_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG7_2 = SDG7_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG7_2 = [x for x in SDG7_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG7_2 = map(lambda SDG7_2:SDG7_2.strip(' '' '),SDG7_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG7_2 = \" \".join(SDG7_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG7_2 = re.sub(r' +', ' ', SDG7_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG7_2 = re.sub(r\"http\\S+\", \"\", SDG7_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 7 - Energy/.Tracking SDG7_The Energy Progress Report 2020_IRENA.pdf.icloud')\n",
    "SDG7_3 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG7_3 = SDG7_3.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG7_3 = [x for x in SDG7_3 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG7_3 = map(lambda SDG7_3:SDG7_3.strip(' '' '),SDG7_3)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG7_3 = \" \".join(SDG7_3)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG7_3 = re.sub(r' +', ' ', SDG7_3)\n",
    "\n",
    "# Delete URL\n",
    "SDG7_3 = re.sub(r\"http\\S+\", \"\", SDG7_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 8 - Decent work/Time to Act for SDG8_ILO.pdf')\n",
    "SDG8_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG8_1 = SDG8_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG8_1 = [x for x in SDG8_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG8_1 = map(lambda SDG8_1:SDG8_1.strip(' '' '),SDG8_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG8_1 = \" \".join(SDG8_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG8_1 = re.sub(r' +', ' ', SDG8_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG8_1 = re.sub(r\"http\\S+\", \"\", SDG8_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 9 - Infrastructure/HLPF 2019 thematic review SDG9.pdf')\n",
    "SDG9_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG9_1 = SDG9_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG9_1 = [x for x in SDG9_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG9_1 = map(lambda SDG9_1:SDG9_1.strip(' '' '),SDG9_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG9_1 = \" \".join(SDG9_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG9_1 = re.sub(r' +', ' ', SDG9_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG9_1 = re.sub(r\"http\\S+\", \"\", SDG9_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 9 - Infrastructure/.UNIDO_STATISTICAL INDICATORS OF INCLUSIVE and sustainable industrialization.pdf.icloud')\n",
    "SDG9_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG9_2 = SDG9_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG9_2 = [x for x in SDG9_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG9_2 = map(lambda SDG9_2:SDG9_2.strip(' '' '),SDG9_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG9_2 = \" \".join(SDG9_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG9_2 = re.sub(r' +', ' ', SDG9_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG9_2 = re.sub(r\"http\\S+\", \"\", SDG9_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 10- reduced inequality/.Human Development Report 2019 UNDP.pdf.icloud')\n",
    "SDG10_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG10_1 = SDG10_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG10_1 = [x for x in SDG10_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG10_1 = map(lambda SDG10_1:SDG10_1.strip(' '' '),SDG10_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG10_1 = \" \".join(SDG10_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG10_1 = re.sub(r' +', ' ', SDG10_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG10_1 = re.sub(r\"http\\S+\", \"\", SDG10_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 10- reduced inequality/.UNDESA_World Social Report 2020.pdf.icloud')\n",
    "SDG10_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG10_2 = SDG10_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG10_2 = [x for x in SDG10_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG10_2 = map(lambda SDG10_2:SDG10_2.strip(' '' '),SDG10_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG10_2 = \" \".join(SDG10_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG10_2 = re.sub(r' +', ' ', SDG10_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG10_2 = re.sub(r\"http\\S+\", \"\", SDG10_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 11 - Cities/HLPF 2018 thematic review SDG11.pdf')\n",
    "SDG11_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG11_1 = SDG11_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG11_1 = [x for x in SDG11_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG11_1 = map(lambda SDG11_1:SDG11_1.strip(' '' '),SDG11_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG11_1 = \" \".join(SDG11_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG11_1 = re.sub(r' +', ' ', SDG11_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG11_1 = re.sub(r\"http\\S+\", \"\", SDG11_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 11 - Cities/.SDG 11 Synthesis report 2018_UN HABITAT.pdf.icloud')\n",
    "SDG11_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG11_2 = SDG11_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG11_2 = [x for x in SDG11_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG11_2 = map(lambda SDG11_2:SDG11_2.strip(' '' '),SDG11_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG11_2 = \" \".join(SDG11_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG11_2 = re.sub(r' +', ' ', SDG11_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG11_2 = re.sub(r\"http\\S+\", \"\", SDG11_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 12 - SCP/HLPF 2018 thematic review SDG12.pdf')\n",
    "SDG12_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG12_1 = SDG12_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG12_1 = [x for x in SDG12_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG12_1 = map(lambda SDG12_1:SDG12_1.strip(' '' '),SDG12_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG12_1 = \" \".join(SDG12_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG12_1 = re.sub(r' +', ' ', SDG12_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG12_1 = re.sub(r\"http\\S+\", \"\", SDG12_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 12 - SCP/.IRP Global Resources Outlook 2019.pdf.icloud')\n",
    "SDG12_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG12_2 = SDG12_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG12_2 = [x for x in SDG12_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG12_2 = map(lambda SDG12_2:SDG12_2.strip(' '' '),SDG12_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG12_2 = \" \".join(SDG12_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG12_2 = re.sub(r' +', ' ', SDG12_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG12_2 = re.sub(r\"http\\S+\", \"\", SDG12_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 13 - Climate/.Emissions Gap repor 2019_UNEP.pdf.icloud')\n",
    "SDG13_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG13_1 = SDG13_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG13_1 = [x for x in SDG13_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG13_1 = map(lambda SDG13_1:SDG13_1.strip(' '' '),SDG13_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG13_1 = \" \".join(SDG13_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG13_1 = re.sub(r' +', ' ', SDG13_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG13_1 = re.sub(r\"http\\S+\", \"\", SDG13_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 13 - Climate/.IPCC 5th Assessment Report.pdf.icloud')\n",
    "SDG13_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG13_2 = SDG13_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG13_2 = [x for x in SDG13_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG13_2 = map(lambda SDG13_2:SDG13_2.strip(' '' '),SDG13_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG13_2 = \" \".join(SDG13_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG13_2 = re.sub(r' +', ' ', SDG13_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG13_2 = re.sub(r\"http\\S+\", \"\", SDG13_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 13 - Climate/.NDC_Outlook_Report_2019_UNDP.pdf.icloud')\n",
    "SDG13_3 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG13_3 = SDG13_3.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG13_3 = [x for x in SDG13_3 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG13_3 = map(lambda SDG13_3:SDG13_3.strip(' '' '),SDG13_3)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG13_3 = \" \".join(SDG13_3)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG13_3 = re.sub(r' +', ' ', SDG13_3)\n",
    "\n",
    "# Delete URL\n",
    "SDG13_3 = re.sub(r\"http\\S+\", \"\", SDG13_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 14 - Life below water/HLPF 2017 Thematic review SDG 14.pdf')\n",
    "SDG14_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG14_1 = SDG14_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG14_1 = [x for x in SDG14_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG14_1 = map(lambda SDG14_1:SDG14_1.strip(' '' '),SDG14_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG14_1 = \" \".join(SDG14_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG14_1 = re.sub(r' +', ' ', SDG14_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG14_1 = re.sub(r\"http\\S+\", \"\", SDG14_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 14 - Life below water/.The_First_Global_Integrated_Marine_Assessment_World_Ocean_Assessment.pdf.icloud')\n",
    "SDG14_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG14_2 = SDG14_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG14_2 = [x for x in SDG14_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG14_2 = map(lambda SDG14_2:SDG14_2.strip(' '' '),SDG14_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG14_2 = \" \".join(SDG14_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG14_2 = re.sub(r' +', ' ', SDG14_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG14_2 = re.sub(r\"http\\S+\", \"\", SDG14_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 15 - Life on land/HLPF 2018 background note SDG 15.pdf')\n",
    "SDG15_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG15_1 = SDG15_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG15_1 = [x for x in SDG15_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG15_1 = map(lambda SDG15_1:SDG15_1.strip(' '' '),SDG15_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG15_1 = \" \".join(SDG15_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG15_1 = re.sub(r' +', ' ', SDG15_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG15_1 = re.sub(r\"http\\S+\", \"\", SDG15_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 15 - Life on land/HLPF 2018 thematic review SDG 15.pdf')\n",
    "SDG15_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG15_2 = SDG15_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG15_2 = [x for x in SDG15_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG15_2 = map(lambda SDG15_2:SDG15_2.strip(' '' '),SDG15_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG15_2 = \" \".join(SDG15_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG15_2 = re.sub(r' +', ' ', SDG15_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG15_2 = re.sub(r\"http\\S+\", \"\", SDG15_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 15 - Life on land/.The State of the Worlds Forest_FAO UNEP.pdf.icloud')\n",
    "SDG15_3 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG15_3 = SDG15_3.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG15_3 = [x for x in SDG15_3 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG15_3 = map(lambda SDG15_3:SDG15_3.strip(' '' '),SDG15_3)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG15_3 = \" \".join(SDG15_3)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG15_3 = re.sub(r' +', ' ', SDG15_3)\n",
    "\n",
    "# Delete URL\n",
    "SDG15_3 = re.sub(r\"http\\S+\", \"\", SDG15_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 16 - Peace/HLPF 2017 Discussion on SDG 16.pdf')\n",
    "SDG16_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG16_1 = SDG16_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG16_1 = [x for x in SDG16_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG16_1 = map(lambda SDG16_1:SDG16_1.strip(' '' '),SDG16_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG16_1 = \" \".join(SDG16_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG16_1 = re.sub(r' +', ' ', SDG16_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG16_1 = re.sub(r\"http\\S+\", \"\", SDG16_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 16 - Peace/.SDG16Progress-Report-2019_Institute for Economics and Peace.pdf.icloud')\n",
    "SDG16_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG16_2 = SDG16_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG16_2 = [x for x in SDG16_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG16_2 = map(lambda SDG16_2:SDG16_2.strip(' '' '),SDG16_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG16_2 = \" \".join(SDG16_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG16_2 = re.sub(r' +', ' ', SDG16_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG16_2 = re.sub(r\"http\\S+\", \"\", SDG16_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SDG 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 17 - Partnerships/.Financing for Sustainable Devleopment report 2020.pdf.icloud')\n",
    "SDG17_1 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG17_1 = SDG17_1.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG17_1 = [x for x in SDG17_1 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG17_1 = map(lambda SDG17_1:SDG17_1.strip(' '' '),SDG17_1)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG17_1 = \" \".join(SDG17_1)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG17_1 = re.sub(r' +', ' ', SDG17_1)\n",
    "\n",
    "# Delete URL\n",
    "SDG17_1 = re.sub(r\"http\\S+\", \"\", SDG17_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 17 - Partnerships/Financing the SDGs Moving from words to Action_Background note_ UNDESA.pdf')\n",
    "SDG17_2 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG17_2 = SDG17_2.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG17_2 = [x for x in SDG17_2 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG17_2 = map(lambda SDG17_2:SDG17_2.strip(' '' '),SDG17_2)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG17_2 = \" \".join(SDG17_2)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG17_2 = re.sub(r' +', ' ', SDG17_2)\n",
    "\n",
    "# Delete URL\n",
    "SDG17_2 = re.sub(r\"http\\S+\", \"\", SDG17_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PDF text \n",
    "raw = parser.from_file('/Users/jadeguisiano/Desktop/Nations Unies/One-Planet/Flagship reports for SDG analysis/SDG 17 - Partnerships/HLPF 2018 thematic review SDG 17.pdf')\n",
    "SDG17_3 = raw['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split string when '/n' occur\n",
    "SDG17_3 = SDG17_3.splitlines()   \n",
    "\n",
    "# Delete empty string\n",
    "SDG17_3 = [x for x in SDG17_3 if x]\n",
    "\n",
    "# Delete all :  ' \n",
    "SDG17_3 = map(lambda SDG17_3:SDG17_3.strip(' '' '),SDG17_3)\n",
    "\n",
    "# Join all string to form just one per PDF\n",
    "SDG17_3 = \" \".join(SDG17_3)\n",
    "\n",
    "# Delete extra white space \n",
    "SDG17_3 = re.sub(r' +', ' ', SDG17_3)\n",
    "\n",
    "# Delete URL\n",
    "SDG17_3 = re.sub(r\"http\\S+\", \"\", SDG17_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
